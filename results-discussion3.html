<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width" />
    <title>My DEEP Learning webpage</title>
    <link href="styles/style.css" rel="stylesheet" />
    <link href="https://fonts.googleapis.com/css?family=Zilla+Slab:400,700|Open+Sans" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.0.0/p5.min.js"></script>
  </head>
  <body>
    <header>
        <h1> DEEP Learning Excercise Website </h1>
        <div id="nav-menu">
          <a href="index.html">EA1 - ml5 Image Classification</a>
          <a href="ea2.html">EA2 - Regression </a>
          <a href="ea3.html" class="current-main">EA3 - Language Model mit RNN</a>
        </div>
        <div class="sub-menu">
            <div></div> <!-- Leerer Platzhalter für nicht ausgewählte Elemente -->
            <div></div> <!-- Leerer Platzhalter für nicht ausgewählte Elemente -->
            <div id="nav-menu">
              <a href="ea3.html">word prediction</a>
              <a href="documentation3.html">Documentation</a>
              <a href="results-discussion3.html" class="current">Results and Discussion</a>
            </div>
        </div>
    </header>
    <main>
        <div id="white-area">
            <h2>Modellarchitektur</h2>
            <p style="font-size:20px;">
            Siehe Dokumentation. Die Modelle sind sehr einfach geworden, weil auf der limitierten Wortmenge keine große Differenz bei der Evaluation erzielt werden konnte und schon die einfachen Modelle sehr lange zum Trainieren gebraucht haben. Teilweise führten schon 5 Layer zum Error, dass die Kapazität der Rechenleistung der Anwendung gesprengt wurde und diese deshalb nicht ausgeführt wurde. Es wurde ebenfalls damit experimentiert, den Originaltext in einzelne Buchstaben aufzuteilen und an ein maschinelles Netz zur Vorhersage des nächsten Buchstabens zu übergeben. Dabei konnten aber keine großen Erfolge erzielt werden, da das Ergebnis schwierig im Kontext zu evaluieren war und die Vorhersage die längste Zeit underfittet war und deshalb meistens einfach den häufigsten Buchstaben "e" ergeben hat, auch nach langem Trainieren. Die next character prediction findet sich noch ausgesternt im Code auf github, wurde aber der Sinnlosigkeit wegen vorerst aufgegeben und findet sich nicht in der finalen Anwendung wieder.
            </p>
            <h2>Evaluation und Vergleich</h2>
            <p style="font-size:20px;">
            Zur Evaluation habe ich zufällige Sätze aus dem Originaltext genommen. In Klammern steht die guess Nummer von (RNN/FFNN) (erstes Wort: Häufigkeit im Ausgangstext): Die (62) ersten (337/ 338) sechs (2023/ 2023) Monate (1357/1357) nach (1/3) der (63/64) Aufnahme (2143/ 2143) Oliver (23/ 23) Twists (2/3) war (1/1) das (2/2) System (1462/1462) in (3/3) vollem (2144/2144) Gange (2145/2145). Dickens (44) ist (3/2) nicht (68/68) nur (71/71) der (62/62) Lieblingsdichter (73/73) seines (1/1) Volkes (1/1), sondern (75/1) er (3/56) ist (69/69) schon (74/74) zu (1/1) Lebzeiten (78/78) in (1/1) allen (78/78) Ländern (2/2) des (1/1) Erdenrunds (82/82) heimisch (1/1) geworden (1/1).
            Von den 35 Wörtern wurde evaluiert, wie oft das nächste Wort unter den ersten k guesses liegt:
            </p>
            <p style="font-size:20px;">
                        K=1     | RNN: 07 richtige guesses | FFNN: 07 richtige guesses (aber nicht die gleichen)
            </p>
            <p style="font-size:20px;">
                        K=5     | RNN: 13 richtige guesses | FFNN: 13 richtige guesses (aber nicht die gleichen)
            </p>
            <p style="font-size:20px;">
                        K=10    | RNN: 13 richtige guesses | FFNN: 13 richtige guesses (aber nicht die gleichen)
            </p>
            <p style="font-size:20px;">
                        K=20    | RNN: 13 richtige guesses | FFNN: 13 richtige guesses (aber nicht die gleichen)
            </p>
            <p style="font-size:20px;">
                        K=100   | RNN: 23 richtige guesses | FFNN: 23 richtige guesses (aber nicht die gleichen)
            </p>
            <p style="font-size:20px;">
            Im Ergebnis liegen beide Modelle etwa auf gleicher Höhe, der loss gleicht sich bis zur zweiten Nachkommastelle, obwohl das FFNN-Modell geringfügig komplexer ist. FFNN trainiert zudem deutlich länger pro Epoche. Die Ergebnisse lassen die Vermutung zu, dass die Modelle noch nicht ausreichend komplex sind: bei seltenen Wörtern wird nur manchmal korrekt das nächste Wort predicted, obwohl es im Ausgangstext nur genau einen Nachfolger gibt. Die Komplexität der Modelle konnte aber wegen der Anwendungskapazitätsgrenzen nicht einfach erhöht werden. Die Modelle sind jedoch bereits ausreichen komplex, um nicht immer der Häufigkeitsliste zu folgen, also nicht immer "und" vorherzusagen. Dies sieht man auch im Evaluationsbeispiel: guess Nummer 78 war einmal "Lebzeiten" und einmal "allen". Unterschiede gibt es scheinbar zufällig bei einzelnen Wörtern. So sagt FFNN korrekt das Wort "sondern" hervor, RNNs guess Nr.75 nur um beim nächsten Wort "er" mit 56:3 weit hinter RNN zurückzuliegen. Anmerkung: Im Debugger der Konsole werden noch viele weitere Informationen ausgegeben, in der "initial frequency list", der Häufigkeitsliste der Wörter im Ausgangstext, liegt "er" auf Platz 5. A(nmerkung: Ein ähnliches Ergebnis bekomme ich auch, wenn ich das Zeitfenster auf 2 oder 3 erhöhe).
            </p>
            <p style="font-size:20px;">
            </p>
            <h2>Datenschutz</h2>
            <p style="font-size:20px;">
            Eine Rekonstruktion des Ausgangstextes ist bei dem semi-guten Modell nicht gut möglich. Ein Datenschutzproblem ergibt sich daraus aber nicht: Dies tritt aus meiner Sicht stattdessen an der Stelle auf, an der ich die Daten zum Trainieren des Maschinellen Netzes übergebe.
            </p>
        </div>
    </main>
  </body>
</html>